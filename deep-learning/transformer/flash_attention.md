âš¡ Summary: The Essence of FlashAttention

FlashAttention is a faster, more memory-efficient implementation of the attention mechanism in Transformer models. It computes exact attention (not approximate) using clever memory and math optimizations.

â¸»

ðŸš€ Key Techniques

1. Tiling
	â€¢	Breaks large attention matrices into smaller blocks (tiles).
	â€¢	Each tile fits into fast GPU shared memory.
	â€¢	Reduces global memory reads/writes and improves speed.

2. Shared Memory
	â€¢	Uses fast, on-chip shared memory instead of slower global memory.
	â€¢	Allows efficient reuse of data like keys and values across threads.

â¸»

ðŸ§  Standard Attention (for context)

Attention(Q, K, V) = softmax(Q Ã— K^T / sqrt(d_k)) Ã— V

	â€¢	Requires computing the full Q Ã— K^T matrix.
	â€¢	Needs large memory, especially for long sequences (O(nÂ²) space).

â¸»

âœ… FlashAttentionâ€™s Softmax Trick

Instead of computing the full attention matrix, FlashAttention processes blocks of keys/values and updates the result incrementally, using a streaming softmax.
For each query row i, it maintains:
	â€¢	m_i: the running maximum logit seen so far.
	â€¢	l_i: the running denominator of softmax (sum of exponentials).
	â€¢	o_i: the running attention output.

Each tile is processed as follows:
	â€¢	Compute partial logits (Q Ã— K_tile^T).
	â€¢	Update m_i, l_i, and o_i using a rescaling trick:
	â€¢	Reweight old values to match the new max.
	â€¢	Add new contributions from the tile.
	â€¢	Final result is numerically stable and memory-efficient.

â¸»

ðŸ’¡ Benefits
	â€¢	Trains models with much longer sequences (4Kâ€“32K tokens).
	â€¢	Reduces memory usage and improves speed by 2â€“4Ã—.
	â€¢	Delivers exact results â€” no approximations.
	â€¢	Used in large models like GPT-4, LLaMA 2, and Mistral.
