ğŸ§  What Is Beam Search?

Beam search keeps track of the top-k most likely sequences at each generation step instead of just the single best one (like greedy decoding).

Think of it as breadth-first search in sequence space, keeping only the best beam_width candidates.

â¸»

ğŸª„ Core Idea:

At each decoding step:
	1.	For each active sequence in the beam, compute logits for the next token
	2.	Expand each by all possible next tokens â†’ many candidate sequences
	3.	Select the top k overall sequences (based on total score)
	4.	Repeat until:
	â€¢	<eos> is generated
	â€¢	Max length is reached

â¸»

ğŸ“Š Example: Beam Width = 2

Say at time t=1 you have:

[<bos>] â†’ ["The", "A"]

At time t=2, suppose you expand each:

"The" â†’ ["The cat", "The dog"]
"A"   â†’ ["A car", "A man"]

Now you rank all 4 and keep the top 2:

["The cat", "A man"]  â† beam size = 2



â¸»

âœ¨ Why Use It?

âœ… Advantages:
	â€¢	Avoids greedy mistakes
	â€¢	Produces more coherent, global sequences
	â€¢	Especially useful in:
	â€¢	Translation
	â€¢	Summarization
	â€¢	Question answering

âš ï¸ Disadvantages:
	â€¢	More computationally expensive (beam width Ã— model calls)
	â€¢	Can still lack diversity (â†’ solutions: diverse beam search, top-k hybrid)

â¸»

ğŸ“ˆ How Is It Scored?

Typically uses log-probabilities:

score = sum(log(P(token_i)))  # across time steps

Optionally normalized:

score /= (sequence_length ** length_penalty)



â¸»

ğŸ§ª Code (HuggingFace style)

model.generate(
    input_ids,
    max_length=50,
    num_beams=5,          # beam width
    early_stopping=True,
    no_repeat_ngram_size=2,
    length_penalty=1.0
)



â¸»

âœ… TL;DR

Beam search explores multiple candidate sequences at each step, balancing likelihood and diversity. Itâ€™s better than greedy, less random than sampling, and often used in high-quality generation tasks.
