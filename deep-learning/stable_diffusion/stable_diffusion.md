## üß† 1. Core Components

Stable Diffusion has three main components:
1. **Autoencoder**: Compresses images to a latent space and reconstructs them.
2. **U-Net**: The neural network that learns to reverse the diffusion process.
3. **Text Encoder** (e.g., CLIP or OpenCLIP): Converts input text prompts into vector embeddings that guide the image generation.

---

## üîÑ 2. Diffusion Process (Training Phase)
- The model learns to denoise a latent vector progressively over several timesteps.
- Starting from a clean image, noise is gradually added to form a sequence x‚ÇÄ ‚Üí x‚ÇÅ ‚Üí ... ‚Üí x‚Çç‚Çú‚Çé, where x‚Çç‚Çú‚Çé is nearly pure noise.
- The U-Net is trained to predict the noise Œµ‚Çç‚Çú‚Çé at each step t, conditioned on the noisy input x‚Çç‚Çú‚Çé and the text embedding.

### Training Objective (simplified):

L = E[|| Œµ_Œ∏(x‚Çç‚Çú‚Çé, t, c) - Œµ ||¬≤]

Where:
- Œµ_Œ∏ is the predicted noise by the U-Net.
- Œµ is the actual noise added.
- c is the conditioning from the text encoder.

---

## üåÄ 3. Latent Space Diffusion (Efficiency Trick)

Instead of applying diffusion directly to full-resolution images (which is costly), Stable Diffusion:
- Compresses the image using a VAE encoder (from 512√ó512√ó3 ‚Üí ~64√ó64√ó4 latent representation).
- Runs the diffusion process in this smaller latent space.
- Decodes the final latent output back to pixel space using the VAE decoder.

This speeds up both training and inference drastically.

---

## üß¨ 4. Sampling (Inference Phase)
- Start from random noise z‚Çç‚Çú‚Çé in the latent space.
- Iteratively denoise it with the U-Net, guided by the text condition, over T steps.
- Each step predicts the noise component, and you update the latent as:

z‚Çç‚Çú‚Çã‚ÇÅ‚Çé = denoise(z‚Çç‚Çú‚Çé, Œµ_Œ∏(z‚Çç‚Çú‚Çé, t, c))

- After T steps, decode the final latent z‚ÇÄ back to image space using the VAE decoder.

---

## üèóÔ∏è Architecture Summary

| Component | Description |
|-----------|-------------|
| VAE Encoder | Compresses image to latent space |
| U-Net | Denoises latent vectors, conditioned on timestep and text |
| Text Encoder | Encodes prompt to conditioning vector |
| VAE Decoder | Reconstructs image from latent space |

The U-Net is enhanced with:
- Cross-Attention between image latents and text embeddings
- Time embeddings to inform the network of the current diffusion step

---

## üîÅ Quick Recap: What Happens During Training?

- You take a clean image x‚ÇÄ.
- You add Gaussian noise progressively to get x‚ÇÅ, x‚ÇÇ, ..., x_T, where x_T is almost pure noise.
- The model learns to predict the noise Œµ_t that was added to x_t (not the denoised image itself).
- This is learned at random steps t, using supervised loss like MSE(Œµ_pred, Œµ_true).

---

## ‚ùì Then What Happens at Inference?

Now you run the process in reverse:
You start from z_T (pure noise in the latent space) and want to recover z‚ÇÄ (the clean latent image).

Here's how it works:

---

## üß™ 1. Start from Random Noise

z_T ~ N(0, I)  # Sampled from Gaussian

---

## üîÑ 2. Iterative Denoising with the U-Net

For each step t = T, T-1, ..., 1:
1. **Predict the noise**:
   The U-Net estimates ŒµÃÇ = Œµ_Œ∏(z_t, t, cond) ‚Äî the noise it thinks is present at step t.
2. **Estimate the previous latent**:
   Use the denoising formula derived from the forward process:

   z_{t-1} = (z_t - Œ≤_t * ŒµÃÇ) / sqrt(1 - Œ≤_t)

   Or more commonly, in its full DDPM-style form (simplified):

   z_{t-1} = Œº_Œ∏(z_t, ŒµÃÇ, t) + œÉ_t * noise

   Where:
   - Œº_Œ∏ is the mean of the reverse process, derived from the predicted noise.
   - œÉ_t is the variance schedule (may be deterministic or stochastic depending on sampler).
   - noise is optionally re-sampled depending on the sampler type (e.g., DDIM, PLMS, DPM).

3. This gives you a slightly less noisy latent: z_{t-1}.

Repeat until z‚ÇÄ.

---

## üß© 3. Decode Final Latent to Image

Finally:

image = vae.decode(z_0)

---

## üîë Key Intuition

- The model never learns to output the clean image.
- It learns to guess the noise, so you can subtract it and move closer to the clean signal.
- You reverse the noise process, step by step, guided by these noise predictions.


# üß† U-Net in Stable Diffusion ‚Äî Architecture Summary

The U-Net in Stable Diffusion is a **ResNet-style convolutional neural network** enhanced with **cross-attention** modules to incorporate text guidance from a separate text encoder (like CLIP).

---

## üè† High-Level Structure

```text
Input (noisy latent z_t)
    ‚Üì
[Downsampling Blocks]
    ‚Üì
[Mid Block (Self-Attn + Cross-Attn)]
    ‚Üì
[Upsampling Blocks]
    ‚Üì
Output (predicted noise ŒµÃÇ)
```

* **Down blocks** reduce spatial resolution and extract features
* **Mid block** captures global structure at the bottleneck
* **Up blocks** reconstruct spatial detail using skip connections
* **Skip connections** link corresponding down & up blocks

---

## üîç Key Components

| Component              | Description                                                        |
| ---------------------- | ------------------------------------------------------------------ |
| **ResBlocks**          | Residual CNN blocks used in down, mid, and up paths                |
| **Self-Attention**     | Helps the model understand spatial dependencies within the latent  |
| **Cross-Attention**    | Injects **text guidance** by attending to the CLIP text embeddings |
| **Timestep Embedding** | Encodes the current denoising step (t) and is injected throughout  |
| **Skip Connections**   | Preserve high-resolution detail between encoder and decoder paths  |

---

## ü§ù Cross-Attention: Injecting Prompt Information

* **Cross-attention modules** are inserted into ResBlocks.
* The **query** comes from image latents, and the **key/value** come from the text embedding.
* This lets the model relate specific regions of the image to specific words in the prompt.

```text
Q = image features (from U-Net)
K = text embeddings (from CLIP)
V = text embeddings (from CLIP)
```

Without cross-attention, the model would not "know" what to generate based on the prompt.

---

## ‚ö†Ô∏è Not a Full Transformer

The U-Net **does not include a full Transformer**. It uses **localized attention layers**:

* The **full Transformer** is only in the **text encoder** (e.g., CLIP).
* Inside the U-Net, cross-attention layers are small **transformer-like modules** added to ResBlocks.

---

## üß¨ Summary

* U-Net is a **CNN with residual connections**, enhanced with **cross-attention** to handle text conditioning.
* It denoises the latent over time using timestep-aware and prompt-aware features.
* It outputs predicted noise (`ŒµÃÇ`) of the same shape as input latent `z_t`.



